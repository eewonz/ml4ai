{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eewonz/ml4ai/blob/main/1_pandas_data_structures_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRcUgP1j5bGE"
      },
      "source": [
        "# Pandas Data Structures Overview\n",
        "In this section, we will discuss the `Series`, `Index`, and `DataFrame` classes. To do so, we will read in a snippet of the CSV file we will work with later. Don't worry about that part yet, though.\n",
        "\n",
        "## About the Data\n",
        "In this notebook, we will be working with 5 rows from the earthquake data collected over September 18, 2018 - October 13, 2018 (obtained from the US Geological Survey (USGS) using the [USGS API](https://earthquake.usgs.gov/fdsnws/event/1/))\n",
        "\n",
        "## Working with NumPy Arrays\n",
        "Let's read in a short CSV file (using `numpy`) for some sample data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "os.listdir()\n"
      ],
      "metadata": {
        "id": "4H2FNK60-Lip",
        "outputId": "f7e89461-dadb-4d61-c329-3df52d32e370",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive"
      ],
      "metadata": {
        "id": "epgfeOYs-L2v",
        "outputId": "c35db055-6ba6-4580-e9cf-71ac191fe6b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd MyDrive/Hands-On-Data-Analysis-with-Pandas-2nd-edition/"
      ],
      "metadata": {
        "id": "3K4lL9Yp-L_7",
        "outputId": "50df2cee-5817-4c15-f51b-06b897469e3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'MyDrive/Hands-On-Data-Analysis-with-Pandas-2nd-edition/'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ch_02"
      ],
      "metadata": {
        "id": "GZB_MVHD-MC2",
        "outputId": "35e7a86e-0d55-4332-8b6f-5b8a8d9f84b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'ch_02'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Hands-On-Data-Analysis-with-Pandas-2nd-edition/ch_02"
      ],
      "metadata": {
        "id": "VsbPugjG-Uzp",
        "outputId": "1e7ba36c-f06d-4a1c-97d3-1060952194ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Hands-On-Data-Analysis-with-Pandas-2nd-edition/ch_02'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "nM2v-5BB_Eqn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_data={\n",
        "    'name':['Haram','Minji','Donghyun','Wonjune'],\n",
        "    'Hobby':['Game','running','soccer','baseball'],\n",
        "    'nationality':['korea','korea','korea','korea']\n",
        "}"
      ],
      "metadata": {
        "id": "V4AwcubP_FHW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_df=pd.DataFrame(team_data)"
      ],
      "metadata": {
        "id": "HZGnGu9X_FQ9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team_df.to_csv('team_members.csv',index=False)"
      ],
      "metadata": {
        "id": "lXR41yFx_FY8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_team_df=pd.read_csv('team_members.csv')"
      ],
      "metadata": {
        "id": "iFCCTeGC_Fdt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loaded_team_df.head())"
      ],
      "metadata": {
        "id": "N8O_EcB2_FiQ",
        "outputId": "dda7ecee-fea5-4c67-c6b7-a139f67e436b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       name     Hobby nationality\n",
            "0     Haram      Game       korea\n",
            "1     Minji   running       korea\n",
            "2  Donghyun    soccer       korea\n",
            "3   Wonjune  baseball       korea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loaded_team_df.describe())"
      ],
      "metadata": {
        "id": "HcS_9Zn3_WQt",
        "outputId": "47bd7427-21d6-4efa-d844-8e1950e9c04c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         name Hobby nationality\n",
            "count       4     4           4\n",
            "unique      4     4           1\n",
            "top     Haram  Game       korea\n",
            "freq        1     1           4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = loaded_team_df['name']\n",
        "hobbies = loaded_team_df['Hobby']"
      ],
      "metadata": {
        "id": "Nt16RkMn_WpA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_team_df.set_index('name',inplace=True)"
      ],
      "metadata": {
        "id": "Avb3auZG_WyD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eirsToAw5bGH",
        "outputId": "b48c6f71-c382-4441-ad1b-3679e2c169a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c68257c7c710>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m data = np.genfromtxt(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m'data/example_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1960\u001b[0m         \u001b[0mfid_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: data/example_data.csv not found."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.genfromtxt(\n",
        "    'data/example_data.csv', delimiter=';',\n",
        "    names=True, dtype=None, encoding='UTF'\n",
        ")\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnfKGbJ9-Kjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TuDA7xXs9z6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghrvTBrB5bGI"
      },
      "source": [
        "We can find the dimensions with the `shape` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "544uFK485bGJ"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox-fIE6j5bGJ"
      },
      "source": [
        "We can find the data types with the `dtype` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVHrYpuz5bGJ"
      },
      "outputs": [],
      "source": [
        "data.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuf00wve5bGK"
      },
      "source": [
        "Each of the entries in the array is a row from the CSV file. NumPy arrays contain a single data type (unlike lists, which allow mixed types); this allows for fast, vectorized operations. When we read in the data, we got an array of `numpy.void` objects, which are created to store flexible types. This is because NumPy has to store several different data types per row: four strings, a float, and an integer. This means we can't take advantage of the performance improvements NumPy provides for single data type objects.\n",
        "\n",
        "Say we want to find the maximum magnitude&mdash;we can use a **[list comprehension](https://www.python.org/dev/peps/pep-0202/)** to select the third index of each row, which is represented as a `numpy.void` object. This makes a list, meaning that we can take the maximum using the `max()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pPEvqxI5bGK"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "max([row[3] for row in data])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIrcpexR5bGK"
      },
      "source": [
        "If we, instead, create a NumPy array for each column, this operation is much easier (and more efficient) to perform. We can use a **[dictionary comprehension](https://www.python.org/dev/peps/pep-0274/)** to make a dictionary where the keys are the column names and the values are NumPy arrays of the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeeDAknh5bGL"
      },
      "outputs": [],
      "source": [
        "array_dict = {\n",
        "    col: np.array([row[i] for row in data])\n",
        "    for i, col in enumerate(data.dtype.names)\n",
        "}\n",
        "array_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iuxvaao5bGL"
      },
      "source": [
        "Grabbing the maximum magnitude is now simply a matter of selecting the `mag` key and calling the `max()` method. This is nearly twice as fast as the list comprehension implementation when dealing with just 5 entries, imagine how much worse the first attempt will perform on large data sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYW1UeCi5bGL"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "array_dict['mag'].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMtOh26_5bGM"
      },
      "source": [
        "However, this representation has other issues. Say we wanted to grab all the information for the earthquake with the maximum magnitude, how would we go about that? We would need to find the index of the maximum and then for each of the keys in the dictionary grab that index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb3y6Hp95bGM"
      },
      "outputs": [],
      "source": [
        "np.array([\n",
        "    value[array_dict['mag'].argmax()]\n",
        "    for key, value in array_dict.items()\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7_E4PbA5bGM"
      },
      "source": [
        "The result is now a NumPy array of strings (our numeric values were converted), and we are now in the format from earlier. Also, consider trying to sort the data by magnitude from smallest to largest. In the first representation, we would have to sort the rows by examining the 3rd index. With the second representation, we would have to determine the order for the indices from the `mag` column, and then sort all the other arrays with those same indices. Clearly, working with several NumPy arrays of different data types at once is a bit cumbersome. However, `pandas` builds on top of NumPy arrays to make this easier. Let's start our exploration of `pandas` with an overview of the data structures.\n",
        "\n",
        "## `Series`\n",
        "The `Series` class provides a data structure for arrays of a single type with some additional functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N7V_anS5bGM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "place = pd.Series(array_dict['place'], name='place')\n",
        "place"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "641XRxki5bGN"
      },
      "source": [
        "Here are some commonly used attributes with `Series` objects:\n",
        "\n",
        "|Attribute | Returns |\n",
        "| --- | --- |\n",
        "| `name` | The name of the `Series` object |\n",
        "| `dtype` | The data type of the `Series` object |\n",
        "| `shape` | Dimensions of the `Series` object in a tuple of the form `(number of rows,)` |\n",
        "| `index` | The `Index` object that is part of the `Series` object |\n",
        "| `values` | The data in the `Series` object |\n",
        "\n",
        "For the most part, `pandas` objects use NumPy arrays for their internal data representations. However, for some data types, `pandas` builds upon NumPy to create its own [arrays](https://pandas.pydata.org/pandas-docs/stable/reference/arrays.html). For this reason, depending on the data type, `values` can either be a `pandas.array` or `numpy.array` object. Therefore, if we need to ensure we get a specific type back, then it is recommended to use the `array` attribute or `to_numpy()` method, respectively, instead of `values`.\n",
        "\n",
        "Now let's see some examples using these attributes.\n",
        "\n",
        "### Getting the name of the series\n",
        "The NumPy array held the name of the data in the `dtype` attribute; here, we can access it directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPIxYcoN5bGN"
      },
      "outputs": [],
      "source": [
        "place.name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lxsRmP85bGN"
      },
      "source": [
        "### Getting the data type\n",
        "A `Series` object holds a single data type. Here it is `'O'` for object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsJ1uonb5bGO"
      },
      "outputs": [],
      "source": [
        "place.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOFGGLGd5bGO"
      },
      "source": [
        "### Getting the dimensions of the series\n",
        "Just as with NumPy, we can use `shape` to get the dimensions as `(rows, columns)`. `Series` objects are a single column, so they only have values for the rows dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-Lrrme85bGO"
      },
      "outputs": [],
      "source": [
        "place.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfgV2GEN5bGP"
      },
      "source": [
        "### Isolating the values from the series\n",
        "This `Series` object is storing its values as a NumPy array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqZIMxDd5bGP"
      },
      "outputs": [],
      "source": [
        "place.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfB2slz25bGP"
      },
      "source": [
        "## `Index`\n",
        "The addition of the `Index` class makes the `Series` class more powerful than a NumPy array. We can get the index from the `index` attribute of a `Series` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3uzRZHl5bGP"
      },
      "outputs": [],
      "source": [
        "place_index = place.index\n",
        "place_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwiaL9IB5bGQ"
      },
      "source": [
        "As with `Series` objects, we can access the underlying data via the `values` attribute. Note that this `Index` object is also built on top of a NumPy array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77WFFPf45bGQ"
      },
      "outputs": [],
      "source": [
        "place_index.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7MmkYQn5bGQ"
      },
      "source": [
        "Here are some commonly used attributes with `Index` objects:\n",
        "\n",
        "|Attribute | Returns |\n",
        "| --- | --- |\n",
        "| `name` | The name of the `Index` object |\n",
        "| `dtype` | The data type of the `Index` object |\n",
        "| `shape` | Dimensions of the `Index` object |\n",
        "| `values` | The data in the `Index` object |\n",
        "| `is_unique` | Check if the `Index` object has all unique values |\n",
        "\n",
        "We can check the type of the underlying data, just like with a `Series` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttHsc_kY5bGQ"
      },
      "outputs": [],
      "source": [
        "place_index.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va4VCMhz5bGR"
      },
      "source": [
        "Same for the dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1y_TD225bGR"
      },
      "outputs": [],
      "source": [
        "place_index.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzD0O2sl5bGR"
      },
      "source": [
        "We can check if the values are unique:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYyb0oks5bGR"
      },
      "outputs": [],
      "source": [
        "place_index.is_unique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdKy3mvn5bGR"
      },
      "source": [
        "With NumPy we can perform arithmetic operations element-wise between arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_j-pHJC5bGS"
      },
      "outputs": [],
      "source": [
        "np.array([1, 1, 1]) + np.array([-1, 0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yElsEygL5bGS"
      },
      "source": [
        "Pandas supports this as well, and the index determines how element-wise operations are performed. With addition, only the matching indices are summed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9o2he4J5bGS"
      },
      "outputs": [],
      "source": [
        "numbers = np.linspace(0, 10, num=5) # makes numpy array([0, 2.5, 5, 7.5, 10])\n",
        "x = pd.Series(numbers) # index is [0, 1, 2, 3, 4]\n",
        "y = pd.Series(numbers, index=pd.Index([1, 2, 3, 4, 5]))\n",
        "x + y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiEb6ba95bGS"
      },
      "source": [
        "We aren't limited to the integer indices of list-like structures, and we can label our rows. The labels can be altered at any time and be things like dates or even another column. In chapter 3, we will discuss how to perform some operations on the index in order to change it. Then, in chapter 4, we will use the index for operations merging data and aggregating it.\n",
        "\n",
        "## `DataFrame`\n",
        "Having a `Series` object for each column is an improvement over the NumPy representation; however, we still have the same problem when wanting to sort based on a value or grab an entire row out. The `DataFrame` gives us a representation of a table formed from many `Series` objects that form the columns and a shared `Index` object that labels the rows. We can create a `DataFrame` object from either of the NumPy representations we were working with earlier (we could also make a `Series` object for each column, but there is no need to do so):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htqqHMbI5bGU"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(array_dict)\n",
        "\n",
        "# this will also work with the first representation\n",
        "# df = pd.DataFrame(data)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIYy-cWs5bGU"
      },
      "source": [
        "We can check the type of the underlying data with `dtypes` (note that it is not `dtype` as with `Series` and `Index` objects since each column will have its own data type):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvCk7RTQ5bGU"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYwQ4pk75bGU"
      },
      "source": [
        "We can get the underlying data with the `values` attribute. Note that this looks very similar to our initial NumPy representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNbLxy5G5bGV"
      },
      "outputs": [],
      "source": [
        "df.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Le661DW5bGV"
      },
      "source": [
        "We can isolate the columns with the `columns` attribute. Notice that the columns are actually an `Index` object just on a different axis (columns are the horizontal index while rows are the vertical index)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNmlOiad5bGV"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7hc6fic5bGV"
      },
      "source": [
        "Here are some commonly used attributes:\n",
        "\n",
        "|Attribute | Returns |\n",
        "| --- | --- |\n",
        "| `dtypes` | The data types of each column |\n",
        "| `shape` | Dimensions of the `DataFrame` object in a tuple of the form `(number of rows, number of columns)` |\n",
        "| `index` | The `Index` object along the rows of the `DataFrame` object |\n",
        "| `columns` | The name of the columns (as an `Index` object) |\n",
        "| `values` | The data in the `DataFrame` object |\n",
        "| `empty` | Check if the `DataFrame` object is empty |\n",
        "\n",
        "The `Index` object along the rows of the dataframe can be accessed via the `index` attribute (just as with `Series` objects):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgwj865v5bGW"
      },
      "outputs": [],
      "source": [
        "df.index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkorODAf5bGW"
      },
      "source": [
        "As with both `Series` and `Index` objects, we can get the dimensions of the dataframe with the `shape` attribute. The result is of the form `(nrows, ncols)`. Our dataframe has 5 rows and 6 columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYlYhIvX5bGW"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkrd7Zvo5bGW"
      },
      "source": [
        "Note that we can also perform arithmetic on dataframes. Pandas will only perform the operation when both the index and column match. Here, we demonstrate addition. Since addition with strings means concatenation, `pandas` concatenated the string columns (`time`, `place`, `magType`, and `alert`) across dataframes. The numeric columns (`mag` and `tsunami`) were summed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHT5rc835bGX"
      },
      "outputs": [],
      "source": [
        "df + df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0fqnNiZ5bGX"
      },
      "source": [
        "<hr>\n",
        "<div>\n",
        "    <a href=\"../ch_01/introduction_to_data_analysis.ipynb\">\n",
        "        <button style=\"float: left;\">&#8592; Chapter 1</button>\n",
        "    </a>\n",
        "    <a href=\"./2-creating_dataframes.ipynb\">\n",
        "        <button style=\"float: right;\">Next Notebook &#8594;</button>\n",
        "    </a>\n",
        "</div>\n",
        "<br>\n",
        "<hr>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}